{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.4/1.5 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.5 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/269.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.0/269.0 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.5.15 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'Myself', 'Sandeep', '.', 'I', \"'m\", 'a', 'student', 'who', 'is', 'enthusiatic', 'in', 'Machine', 'Learning', '...', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=\"Hi! Myself Sandeep. I'm a student who is enthusiatic in Machine Learning...!\"\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathing: bath\n",
      "better: good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "print('bathing:',lem.lemmatize(\"bathing\",pos='v'))\n",
    "print('better:',lem.lemmatize(\"better\",pos='a'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'talk', 'bath', 'breath', 'exhaust']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "sentence='Walking Talking Bathing Breathing Exhausting'\n",
    "words=sentence.split()\n",
    "stemmed_words=[ps.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ps=LancasterStemmer()\n",
    "sentence='shaving'\n",
    "words=sentence.split()\n",
    "stemmed_words=[ps.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer=SnowballStemmer(language='english')\n",
    "words_to_stem=['Sandeep','ML Batch','Interesting','Happy','Good']\n",
    "stemmed_words=[stemmer.stem(word) for word in words_to_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['Sandeep', 'ML Batch', 'Interesting', 'Happy', 'Good']\n",
      "Stemmed words: ['sandeep', 'ml batch', 'interest', 'happi', 'good']\n"
     ]
    }
   ],
   "source": [
    "print('Original Words:',words_to_stem)\n",
    "print('Stemmed words:',stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dank'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer as ss\n",
    "ss.languages\n",
    "stemmer=SnowballStemmer('german')\n",
    "stemmer.stem(word='Danke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Kind/NNP)\n",
      "  people/NNS\n",
      "  always/RB\n",
      "  get/VBP\n",
      "  hurt/VBN\n",
      "  atlast/NN\n",
      "  ./.\n",
      "  worried/VBN\n",
      "  sad/JJ\n",
      "  life/NN\n",
      "  always/RB\n",
      "  pretend/VBP\n",
      "  happy/JJ\n",
      "  reality/NN\n",
      "  n't/RB\n",
      "  try/VB\n",
      "  make/VB\n",
      "  others/NNS\n",
      "  happy/JJ\n",
      "  happy/JJ\n",
      "  ./.\n",
      "  n't/RB\n",
      "  get/VB\n",
      "  whatever/WDT\n",
      "  want/NN\n",
      "  ./.\n",
      "  go/VB\n",
      "  flow/JJ\n",
      "  happy/JJ\n",
      "  ./.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Kind', 'NNP'),\n",
       " ('people', 'NNS'),\n",
       " ('always', 'RB'),\n",
       " ('get', 'VBP'),\n",
       " ('hurt', 'VBN'),\n",
       " ('atlast', 'NN'),\n",
       " ('.', '.'),\n",
       " ('worried', 'VBN'),\n",
       " ('sad', 'JJ'),\n",
       " ('life', 'NN'),\n",
       " ('always', 'RB'),\n",
       " ('pretend', 'VBP'),\n",
       " ('happy', 'JJ'),\n",
       " ('reality', 'NN'),\n",
       " (\"n't\", 'RB'),\n",
       " ('try', 'VB'),\n",
       " ('make', 'VB'),\n",
       " ('others', 'NNS'),\n",
       " ('happy', 'JJ'),\n",
       " ('happy', 'JJ'),\n",
       " ('.', '.'),\n",
       " (\"n't\", 'RB'),\n",
       " ('get', 'VB'),\n",
       " ('whatever', 'WDT'),\n",
       " ('want', 'NN'),\n",
       " ('.', '.'),\n",
       " ('go', 'VB'),\n",
       " ('flow', 'JJ'),\n",
       " ('happy', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk\n",
    "\n",
    "text = \"Kind people always gets hurt atlast. He is worried about his sad life and always pretend to be happy but the reality isn't he tries to make others happy but he is not happy. He doesn't get whatever he wants. So just go on the flow to be happy.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "pos_tags = nltk.pos_tag(lemmatized_tokens)\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)\n",
    "lemmatized_tokens\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
